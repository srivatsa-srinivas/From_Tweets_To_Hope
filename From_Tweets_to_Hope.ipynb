{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **From Tweets to Hope: A Data-Driven Approach to Supporting the Needy in America**\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ay5ylkN9DEgr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_6FAg9uZsAJ"
      },
      "outputs": [],
      "source": [
        "# Installs the Python package \"vaderSentiment\" via pip, a package installer for Python.\n",
        "!pip install vaderSentiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUsxclX4IKAP"
      },
      "outputs": [],
      "source": [
        "# Installs the Python package \"flair\" via pip, a package installer for Python.\n",
        "!pip install flair"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4s3Uh6p0sHy"
      },
      "outputs": [],
      "source": [
        "# Installs the Python package \"chart_studio\" via pip, a package installer for Python.\n",
        "!pip install chart_studio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQcM_jcw0uF1"
      },
      "outputs": [],
      "source": [
        "# Installs the Python package \"plotly\" via pip, a package installer for Python.\n",
        "!pip install plotly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z6c3c4Lk0vOz"
      },
      "outputs": [],
      "source": [
        "#  Installs the Python package \"geopandas\" via pip, a package installer for Python.\n",
        "!pip install geopandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOmJZk7uZjPe"
      },
      "outputs": [],
      "source": [
        "import tweepy\n",
        "import re\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import nltk\n",
        "import flair\n",
        "import matplotlib.pyplot as plt\n",
        "import chart_studio.plotly as py\n",
        "import plotly.offline as po\n",
        "import plotly.graph_objs as pg\n",
        "\n",
        "from textblob import TextBlob\n",
        "from flair.models import TextClassifier\n",
        "from flair.data import Sentence\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from nltk import FreqDist\n",
        "from collections import defaultdict\n",
        "from google.colab import files\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# A dataset used for tokenizing text\n",
        "nltk.download('punkt')\n",
        "\n",
        "# A dataset used for part-of-speech tagging\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Downloads the 'stopwords' corpus, which is a list of commonly used words (such as \"the\", \"and\", etc.) that are usually removed from text during pre-processing\n",
        "nltk.download('stopwords')\n",
        "nltk_stops = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "# Downloads the 'wordnet' and 'omw-1.4' corpus, which is used for lemmatization\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "wnl = nltk.WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "2KHcRKSoaBYb"
      },
      "outputs": [],
      "source": [
        "CONSUMER_KEY = # Consumer Key\n",
        "CONSUMER_SECRET = # Consumer Secret\n",
        "OAUTH_TOKEN = # Oauth Token\n",
        "OAUTH_TOKEN_SECRET = # Oauth Token Secret\n",
        "\n",
        "\n",
        "# Authenticate Twitter API credentials\n",
        "auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
        "auth.set_access_token(OAUTH_TOKEN, OAUTH_TOKEN_SECRET)\n",
        "api = tweepy.API(auth)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g87LHEyVJv8j"
      },
      "source": [
        "# **Data Extraction**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1) Needy**"
      ],
      "metadata": {
        "id": "ahHqAzc3Ll38"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8_ivWx1Jv8j"
      },
      "outputs": [],
      "source": [
        "# Collect tweets\n",
        "needyTweets = {}\n",
        "geocode = '\"39.8,-95.583068847656,2500km\"'\n",
        "# Loop through tweets containing specific keywords and within a specific geographic area\n",
        "for tweet in tweepy.Cursor(api.search_tweets, q = \"UrgentHelp OR helpneeded OR Needmoneyforsurgery\\\n",
        "                                                   OR DonateForACause OR FeedTheHungry OR Homelessoutreach\\\n",
        "                                                   OR FeedTheHungry OR Homelessoutreach\\\n",
        "                                                   OR EarthquakeRelief OR CycloneRelief OR GoFundMe\\\n",
        "                                                   OR gofundmedonations OR 4charity -filter:retweets\",\n",
        "                           geocode = geocode,\n",
        "                           lang = 'en',\n",
        "                           tweet_mode = 'extended',\n",
        "                           count = 100).items(10000):\n",
        "  # Removing retweets\n",
        "  if not tweet.retweeted:\n",
        "    # Create dictionary of tweet details and store in 'needyTweets' dictionary using tweet id as key\n",
        "    tweetDetails = {\n",
        "        'name': tweet.user.screen_name,\n",
        "        'tweetID': tweet.id,\n",
        "        'location': tweet.user.location,\n",
        "        'full_text': tweet.full_text\n",
        "    }\n",
        "    needyTweets[tweet.id] = tweetDetails"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2) Donor**"
      ],
      "metadata": {
        "id": "AyQROIaPKMd-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect tweets\n",
        "donorTweets = {}\n",
        "# Represents Latitute and Longitue along with 2500 km Radius.\n",
        "geocode = '\"39.8,-95.583068847656,2500km\"'\n",
        "# Iterate through the Twitter API search results\n",
        "for tweet in tweepy.Cursor(api.search_tweets, q = \"charitysector OR mainedayofgiving OR CharitableGiving\\\n",
        "                                                   OR CommunityService OR MakingADifference OR givingbacktothecommunity\\\n",
        "                                                   OR CharityWork OR Volunteering OR HumanityFirst OR DigitalFundraising\\\n",
        "                                                   OR GivingBack OR NGO OR donationsappreciated OR blooddonor\\\n",
        "                                                   OR plasmadonor OR organdonor -filter:retweets\",\n",
        "                           geocode = geocode,\n",
        "                           lang = 'en',\n",
        "                           tweet_mode = 'extended',\n",
        "                           count=100).items(10000):\n",
        "\n",
        "  # Removing retweets\n",
        "  if not tweet.retweeted:\n",
        "    # Extract tweet details and add to donorTweets dictionary\n",
        "    tweetDetails = {\n",
        "        'name': tweet.user.screen_name,\n",
        "        'tweetID': tweet.id,\n",
        "        'location': tweet.user.location,\n",
        "        'full_text': tweet.full_text\n",
        "    }\n",
        "    # Add the tweet to the donorTweets dictionary with tweet ID as key\n",
        "    donorTweets[tweet.id] = tweetDetails"
      ],
      "metadata": {
        "id": "vTkqWiMHKNS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWtmSFt2N2Nm"
      },
      "source": [
        "# **Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZ5QYMaRN1wN"
      },
      "outputs": [],
      "source": [
        "# Actual Data Pre-processing is done here.\n",
        "def preprocessText(text):\n",
        "    # Tokenize the text\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    # Convert to lowercase\n",
        "    tokens = [token.lower() for token in tokens]\n",
        "    # Consider strings containing only alphabets and numbers\n",
        "    words = [w for w in tokens if w.isalnum()]\n",
        "    # Remove stop words\n",
        "    words = [word for word in words if word not in nltk_stops]\n",
        "    # Lemmatize words\n",
        "    words = [wnl.lemmatize(word) for word in words]\n",
        "    # Return processed text\n",
        "    return words"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1) Needy**"
      ],
      "metadata": {
        "id": "cFAvFhRuLLaQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6wUrzQeJv8k"
      },
      "outputs": [],
      "source": [
        "# Pre-process and Clean-up tweets\n",
        "# A dictionary to store cleaned tweets\n",
        "cleaned_needy_tweets = {}\n",
        "wordListNeedy = []\n",
        "for tweetId, tweetDetails in needyTweets.items():\n",
        "    # remove URLs\n",
        "    cleaned_tweet = re.sub(r'http\\S+', '', tweetDetails['full_text'])\n",
        "    # remove user mentions\n",
        "    cleaned_tweet = re.sub(r'@\\S+', '', cleaned_tweet)\n",
        "    # remove hashtags\n",
        "    cleaned_tweet = re.sub(r'#\\S+', '', cleaned_tweet)\n",
        "    # remove leading/trailing white space\n",
        "    cleaned_tweet = cleaned_tweet.strip()\n",
        "    # only keep non-empty tweets\n",
        "    if cleaned_tweet:\n",
        "        cleaned_needy_tweets[tweetId] = cleaned_tweet\n",
        "        wordListNeedy.extend(preprocessText(cleaned_tweet))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2) Donor**"
      ],
      "metadata": {
        "id": "3v9mho_XLNB2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-process and Clean-up tweets\n",
        "cleaned_donor_tweets = {}\n",
        "wordListDonor = []\n",
        "for tweetId, tweetDetails in donorTweets.items():\n",
        "    # remove URLs\n",
        "    cleaned_tweet = re.sub(r'http\\S+', '', tweetDetails['full_text'])\n",
        "    # remove user mentions\n",
        "    cleaned_tweet = re.sub(r'@\\S+', '', cleaned_tweet)\n",
        "    # remove hashtags\n",
        "    cleaned_tweet = re.sub(r'#\\S+', '', cleaned_tweet)\n",
        "    # remove leading/trailing white space\n",
        "    cleaned_tweet = cleaned_tweet.strip()\n",
        "    # only keep non-empty tweets\n",
        "    if cleaned_tweet:\n",
        "        cleaned_donor_tweets[tweetId] = cleaned_tweet\n",
        "        wordListDonor.extend(preprocessText(cleaned_tweet))"
      ],
      "metadata": {
        "id": "uBNLPjmWKznQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uT69iOJvTMTp"
      },
      "source": [
        "# **Frequency Distribution**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1) Needy**"
      ],
      "metadata": {
        "id": "kVyeDojpLApc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlNkp-9RRARw"
      },
      "outputs": [],
      "source": [
        "# Calculate the frequency distribution of words in the cleaned tweets\n",
        "frequentWordListNeedy = nltk.FreqDist(wordListNeedy)\n",
        "# Get the 30 most common words from the frequency distribution\n",
        "importantWordListNeedy = frequentWordListNeedy.most_common(30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFruWRuNQ6Nv"
      },
      "outputs": [],
      "source": [
        "# Create a list of needy keywords from the most common words\n",
        "needy_keywords = [word for word, count in importantWordListNeedy]\n",
        "\n",
        "# Filter the cleaned tweets to get only those that contain the needy keywords\n",
        "filtered_needy_tweets_freq = {}\n",
        "for tweetId, tweet in cleaned_needy_tweets.items():\n",
        "    if any(keyword in tweet.lower() for keyword in needy_keywords):\n",
        "        filtered_needy_tweets_freq[tweetId] = tweet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2) Donor**"
      ],
      "metadata": {
        "id": "xMuopuRALC5q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the frequency distribution of words in the cleaned tweets\n",
        "frequentWordListDonor = nltk.FreqDist(wordListDonor)\n",
        "# Get the 30 most common words from the frequency distribution\n",
        "importantWordListDonor = frequentWordListDonor.most_common(30)"
      ],
      "metadata": {
        "id": "ON3rt5PmK8Wh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify donor tweets\n",
        "donor_keywords = [word for word, count in importantWordListDonor]\n",
        "\n",
        "# Filter the cleaned tweets to get only those that contain the donor keywords\n",
        "filtered_donor_tweets_freq = {}\n",
        "for tweetId, tweet in cleaned_donor_tweets.items():\n",
        "    if any(keyword in tweet.lower() for keyword in donor_keywords):\n",
        "        filtered_donor_tweets_freq[tweetId] = tweet"
      ],
      "metadata": {
        "id": "r08QaEIeK-sN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dyt0HlGQJv8k"
      },
      "source": [
        "# **Named Entity Recognition**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1) Needy**"
      ],
      "metadata": {
        "id": "aGzdqlIpL5j6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1PWD8RWJv8k"
      },
      "outputs": [],
      "source": [
        "# Define relevant named entity types\n",
        "relevant_entities = [\"ORG\", \"PERSON\", \"MONEY\"]\n",
        "\n",
        "# Create an empty dictionary to store filtered tweets with relevant named entities\n",
        "filtered_needy_tweets_ner = {}\n",
        "\n",
        "# Iterate over each tweet and perform NER\n",
        "for tweetId, tweet in filtered_needy_tweets_freq.items():\n",
        "    # Create a Doc object of the tweet using spaCy\n",
        "    doc = nlp(tweet)\n",
        "    # Extract named entities of relevant types\n",
        "    entities = [ent for ent in doc.ents if ent.label_ in relevant_entities]\n",
        "    # If there are no relevant entities in the tweet, skip to the next tweet\n",
        "    if not entities:\n",
        "        continue\n",
        "    # Otherwise, store the tweet in the filtered_needy_tweets_ner dictionary\n",
        "    filtered_needy_tweets_ner[tweetId] = tweet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2) Donor**"
      ],
      "metadata": {
        "id": "ZrGLak0uL_tS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define relevant named entity types\n",
        "relevant_entities = [\"ORG\", \"PERSON\", \"MONEY\"]\n",
        "\n",
        "# Create an empty dictionary to store filtered tweets with relevant named entities\n",
        "filtered_donor_tweets_ner = {}\n",
        "\n",
        "# Iterate over each tweet and perform NER\n",
        "for tweetId, tweet in filtered_donor_tweets_freq.items():\n",
        "    # Create a Doc object of the tweet using spaCy\n",
        "    doc = nlp(tweet)\n",
        "    # Extract named entities of relevant types\n",
        "    entities = [ent for ent in doc.ents if ent.label_ in relevant_entities]\n",
        "    # If there are no relevant entities in the tweet, skip to the next tweet\n",
        "    if not entities:\n",
        "        continue\n",
        "    # Otherwise, store the tweet in the filtered_donor_tweets_ner dictionary\n",
        "    filtered_donor_tweets_ner[tweetId] = tweet"
      ],
      "metadata": {
        "id": "QN5Kwlf5MAI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrZcW8qtJv8k"
      },
      "source": [
        "# **POS Tagging**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1) Needy**"
      ],
      "metadata": {
        "id": "UbueWNoBMJHT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkP6PXykJv8k"
      },
      "outputs": [],
      "source": [
        "# Nouns (NN):       \"donation\", \"charity\", \"help\", \"support\"\n",
        "# Verbs (VB):       \"donate,\" \"give,\" \"support,\" \"help\" are frequently used in these tweets.\n",
        "# Adjectives (JJ):  \"generous,\" \"kind,\" \"grateful,\"  \"thankful\" are used to express\n",
        "#                     gratitude towards donors and supporters.\n",
        "# Pronouns (PRP):   \"we\", \"us\" and \"you\" are used to refer to the individuals\n",
        "#                     and organizations involved in the charitable efforts.\n",
        "# Adverbs (RB):     \"kindly,\" \"graciously,\" and \"generously\" are used to describe\n",
        "#                     the manner in which donations and support are given.\n",
        "\n",
        "relevant_tags = [\"NN\", \"VB\", \"JJ\", \"PRP\", \"RB\"]\n",
        "\n",
        "# Define POS tagging function\n",
        "def pos_tagging(text):\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tags = nltk.pos_tag(tokens)\n",
        "    return tags\n",
        "\n",
        "# Create empty dictionary to store filtered tweets\n",
        "filtered_needy_tweets_pos = {}\n",
        "\n",
        "# Iterate over the tweets that contain relevant entities\n",
        "for tweetId, tweet in filtered_needy_tweets_ner.items():\n",
        "\n",
        "    # Perform POS tagging on the tweet text\n",
        "    tags = pos_tagging(tweet)\n",
        "\n",
        "    # Check if the tweet contains any relevant POS tags\n",
        "    if any(tag[1] in relevant_tags for tag in tags):\n",
        "        filtered_needy_tweets_pos[tweetId] = tweet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2) Donor**"
      ],
      "metadata": {
        "id": "Ofm9GYEKMNXS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Nouns (NN):       \"donation\", \"charity\", \"help\", \"support\"\n",
        "# Verbs (VB):       \"donate,\" \"give,\" \"support,\" \"help\" are frequently used in these tweets.\n",
        "# Adjectives (JJ):  \"generous,\" \"kind,\" \"grateful,\"  \"thankful\" are used to express\n",
        "#                     gratitude towards donors and supporters.\n",
        "# Pronouns (PRP):   \"we\", \"us\" and \"you\" are used to refer to the individuals\n",
        "#                     and organizations involved in the charitable efforts.\n",
        "# Adverbs (RB):     \"kindly,\" \"graciously,\" and \"generously\" are used to describe\n",
        "#                     the manner in which donations and support are given.\n",
        "\n",
        "relevant_tags = [\"NN\", \"VB\", \"JJ\", \"PRP\", \"RB\"]\n",
        "\n",
        "# POS tagging function\n",
        "def pos_tagging(text):\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tags = nltk.pos_tag(tokens)\n",
        "    return tags\n",
        "\n",
        "# Create empty dictionary to store filtered tweets\n",
        "filtered_donor_tweets_pos = {}\n",
        "\n",
        "# Iterate over the tweets that contain relevant entities\n",
        "for tweetId, tweet in filtered_donor_tweets_ner.items():\n",
        "\n",
        "    # Perform POS tagging on the tweet text\n",
        "    tags = pos_tagging(tweet)\n",
        "\n",
        "    # Check if the tweet contains any relevant POS tags\n",
        "    if any(tag[1] in relevant_tags for tag in tags):\n",
        "        filtered_donor_tweets_pos[tweetId] = tweet"
      ],
      "metadata": {
        "id": "Tu3FX1XwMIEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFcbXSkkJv8l"
      },
      "source": [
        "# **Sentiment Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1) Needy**"
      ],
      "metadata": {
        "id": "H-9FypMWMug8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtIIufpgJv8l"
      },
      "source": [
        "**a) TextBlob**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YY0Po6VSJv8l"
      },
      "outputs": [],
      "source": [
        "textblob_scores_needy = {}\n",
        "for tweetId, tweet in filtered_needy_tweets_pos.items():\n",
        "    # Calculate TextBlob sentiment polarity score for each tweet\n",
        "    textblob_score = TextBlob(tweet).sentiment.polarity\n",
        "    # Add the score to the textblob_scores_needy dictionary with tweetId as key\n",
        "    textblob_scores_needy[tweetId] = textblob_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQWTwbmMJv8l"
      },
      "source": [
        "**b) Vader**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLREqipuJv8l"
      },
      "outputs": [],
      "source": [
        "vader_scores_needy = {}\n",
        "for tweetId, tweet in filtered_needy_tweets_pos.items():\n",
        "    # Initialize the SentimentIntensityAnalyzer\n",
        "    vader_analyzer = SentimentIntensityAnalyzer()\n",
        "    # Calculate the Vader sentiment compound score for each tweet\n",
        "    vader_score = vader_analyzer.polarity_scores(tweet)['compound']\n",
        "    # Add the score to the vader_scores_needy dictionary with tweetId as key\n",
        "    vader_scores_needy[tweetId] = vader_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5_MWkT1Jv8l"
      },
      "source": [
        "**c) Flair**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZqDPB-SJv8l"
      },
      "outputs": [],
      "source": [
        "# Load pre-trained sentiment classifier from Flair\n",
        "flair_classifier = TextClassifier.load('en-sentiment')\n",
        "\n",
        "flair_scores_needy = {}\n",
        "for tweetId, tweet in filtered_needy_tweets_pos.items():\n",
        "    # Create a Flair Sentence object from the tweet\n",
        "    flair_sentence = Sentence(tweet)\n",
        "    # Use the Flair classifier to predict the sentiment of the sentence\n",
        "    flair_classifier.predict(flair_sentence)\n",
        "    # flair_scores_needy[tweetId] = flair_sentence.labels[0].score\n",
        "    # Store the sentiment label with highest confidence score as the sentiment score for the tweet\n",
        "    flair_scores_needy[tweetId] = flair_sentence.labels[0].value"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Classifying Positive and Negative Tweets:**"
      ],
      "metadata": {
        "id": "CQ0KsXWOMcNZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAh-CpVqJv8l"
      },
      "outputs": [],
      "source": [
        "# Initialize dictionaries to store positive and negative tweets based on TextBlob, VADER, and Flair scores\n",
        "\n",
        "positive_tweets_needy_textblob = {}\n",
        "negative_tweets_needy_textblob = {}\n",
        "\n",
        "positive_tweets_needy_vader = {}\n",
        "negative_tweets_needy_vader = {}\n",
        "\n",
        "positive_tweets_needy_flair = {}\n",
        "negative_tweets_needy_flair = {}\n",
        "\n",
        "# Iterate through each tweet and score them based on their TextBlob, VADER, and Flair sentiment scores\n",
        "for tweetId, tweet in filtered_needy_tweets_pos.items():\n",
        "    # Add tweet to the positive dictionary if its TextBlob score is above 0.25, else add it to negative dictionary\n",
        "    if textblob_scores_needy[tweetId] > 0.25:\n",
        "        positive_tweets_needy_textblob[tweetId] = tweet\n",
        "    else:\n",
        "        negative_tweets_needy_textblob[tweetId] = tweet\n",
        "\n",
        "    # Add tweet to the positive dictionary if its VADER score is above 0.05, else add it to negative dictionary\n",
        "    if vader_scores_needy[tweetId] > 0.05:\n",
        "        positive_tweets_needy_vader[tweetId] = tweet\n",
        "    else:\n",
        "        negative_tweets_needy_vader[tweetId] = tweet\n",
        "\n",
        "    # Add tweet to the positive dictionary if its Flair score is 'POSITIVE', else add it to negative dictionary\n",
        "    if flair_scores_needy[tweetId] == 'POSITIVE':\n",
        "        positive_tweets_needy_flair[tweetId] = tweet\n",
        "    else:\n",
        "        negative_tweets_needy_flair[tweetId] = tweet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2) Donor**"
      ],
      "metadata": {
        "id": "kFOFR3kMMzpx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqQv6B3AM6Qn"
      },
      "source": [
        "**a) TextBlob**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ld4HDL6mM6Qo"
      },
      "outputs": [],
      "source": [
        "textblob_scores_donor = {}\n",
        "for tweetId, tweet in filtered_donor_tweets_pos.items():\n",
        "    # Calculate TextBlob sentiment polarity score for each tweet\n",
        "    textblob_score = TextBlob(tweet).sentiment.polarity\n",
        "    # Add the score to the textblob_scores_donor dictionary with tweetId as key\n",
        "    textblob_scores_donor[tweetId] = textblob_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXYD5WpjM6Qo"
      },
      "source": [
        "**b) Vader**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SUsEgR_0M6Qo"
      },
      "outputs": [],
      "source": [
        "vader_scores_donor = {}\n",
        "for tweetId, tweet in filtered_donor_tweets_pos.items():\n",
        "    # Initialize the SentimentIntensityAnalyzer\n",
        "    vader_analyzer = SentimentIntensityAnalyzer()\n",
        "    # Calculate the Vader sentiment compound score for each tweet\n",
        "    vader_score = vader_analyzer.polarity_scores(tweet)['compound']\n",
        "    # Add the score to the vader_scores_donor dictionary with tweetId as key\n",
        "    vader_scores_donor[tweetId] = vader_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7n_47u5QM6Qo"
      },
      "source": [
        "**c) Flair**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ziwv8stxM6Qp"
      },
      "outputs": [],
      "source": [
        "# Load pre-trained sentiment classifier from Flair\n",
        "flair_classifier = TextClassifier.load('en-sentiment')\n",
        "\n",
        "flair_scores_donor = {}\n",
        "for tweetId, tweet in filtered_donor_tweets_pos.items():\n",
        "    # Create a Flair Sentence object from the tweet\n",
        "    flair_sentence = Sentence(tweet)\n",
        "    # Use the Flair classifier to predict the sentiment of the sentence\n",
        "    flair_classifier.predict(flair_sentence)\n",
        "    # flair_scores_donor[tweetId] = flair_sentence.labels[0].score\n",
        "    # Store the sentiment label with highest confidence score as the sentiment score for the tweet\n",
        "    flair_scores_donor[tweetId] = flair_sentence.labels[0].value"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Classifying Positive and Negative Tweets:**"
      ],
      "metadata": {
        "id": "YClm7ooVNLyc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKqCkVPSM6Qp"
      },
      "outputs": [],
      "source": [
        "# Initialize dictionaries to store positive and negative tweets based on TextBlob, VADER, and Flair scores\n",
        "\n",
        "positive_tweets_donor_textblob = {}\n",
        "negative_tweets_donor_textblob = {}\n",
        "\n",
        "positive_tweets_donor_vader = {}\n",
        "negative_tweets_donor_vader = {}\n",
        "\n",
        "positive_tweets_donor_flair = {}\n",
        "negative_tweets_donor_flair = {}\n",
        "\n",
        "# Iterate through each tweet and score them based on their TextBlob, VADER, and Flair sentiment scores\n",
        "for tweetId, tweet in filtered_donor_tweets_pos.items():\n",
        "    # Add tweet to the positive dictionary if its TextBlob score is above 0.25, else add it to negative dictionary\n",
        "    if textblob_scores_donor[tweetId] > 0.25:\n",
        "        positive_tweets_donor_textblob[tweetId] = tweet\n",
        "    else:\n",
        "        negative_tweets_donor_textblob[tweetId] = tweet\n",
        "\n",
        "    # Add tweet to the positive dictionary if its VADER score is above 0.05, else add it to negative dictionary\n",
        "    if vader_scores_donor[tweetId] > 0.05:\n",
        "        positive_tweets_donor_vader[tweetId] = tweet\n",
        "    else:\n",
        "        negative_tweets_donor_vader[tweetId] = tweet\n",
        "\n",
        "    # Add tweet to the positive dictionary if its Flair score is 'POSITIVE', else add it to negative dictionary\n",
        "    if flair_scores_donor[tweetId] == 'POSITIVE':\n",
        "        positive_tweets_donor_flair[tweetId] = tweet\n",
        "    else:\n",
        "        negative_tweets_donor_flair[tweetId] = tweet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_QoatXIK_Ms"
      },
      "source": [
        "# **Metrics**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**a) Bar Graphs**"
      ],
      "metadata": {
        "id": "UfGsss2iQgUc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Positive Tweets of Needy & (Volunteer/Donor) Groups for TextBlob, Vader, Flair:**"
      ],
      "metadata": {
        "id": "bvdHyqpsOUzL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5nOP3L8KxbI"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot Textblob sentiment analysis results\n",
        "fig, ax = plt.subplots()\n",
        "ax.bar(['Needy', 'Volunteer'], [len(positive_tweets_needy_textblob), len(positive_tweets_donor_textblob)])\n",
        "ax.set_ylabel('Number of Tweets')\n",
        "ax.set_title('Textblob - Twitter Analysis Results')\n",
        "plt.show()\n",
        "\n",
        "# Plot Vader sentiment analysis results\n",
        "fig, ax = plt.subplots()\n",
        "ax.bar(['Needy', 'Volunteer'], [len(positive_tweets_needy_vader), len(positive_tweets_donor_vader)])\n",
        "ax.set_ylabel('Number of Tweets')\n",
        "ax.set_title('Vader - Twitter Analysis Results')\n",
        "plt.show()\n",
        "\n",
        "# Plot Flair sentiment analysis results\n",
        "fig, ax = plt.subplots()\n",
        "ax.bar(['Needy', 'Volunteer'], [len(positive_tweets_needy_flair), len(positive_tweets_donor_flair)])\n",
        "ax.set_ylabel('Number of Tweets')\n",
        "ax.set_title('Flair - Twitter Analysis Results')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**b) Pie Charts**"
      ],
      "metadata": {
        "id": "UyqfzLBiQk6U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1) Sentiment Distribution of Needy**"
      ],
      "metadata": {
        "id": "NNtw9bRxPn_u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzbHR8ZLqryt"
      },
      "outputs": [],
      "source": [
        "# Text Blob - Needy Pie Chart Sentiment\n",
        "totalNeedy_Textblob = len(positive_tweets_needy_textblob) + len(negative_tweets_needy_textblob)\n",
        "labels = ['Positive', 'Negative']\n",
        "sizes = [format(100 * len(positive_tweets_needy_textblob) / totalNeedy_Textblob), format(100 * len(negative_tweets_needy_textblob) / totalNeedy_Textblob)]\n",
        "colors = ['green', 'red']\n",
        "# Create a pie chart\n",
        "plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
        "# Add a title\n",
        "plt.title('TextBlob - Needy Sentiment distribution')\n",
        "# Show the chart\n",
        "plt.show()\n",
        "\n",
        "print()\n",
        "\n",
        "# Vader - Needy Pie Chart Sentiment\n",
        "totalNeedy_Vader = len(positive_tweets_needy_vader) + len(negative_tweets_needy_vader)\n",
        "labels = ['Positive', 'Negative']\n",
        "sizes = [format(100 * len(positive_tweets_needy_vader) / totalNeedy_Vader), format(100 * len(negative_tweets_needy_vader) / totalNeedy_Vader)]\n",
        "colors = ['green', 'red']\n",
        "# Create a pie chart\n",
        "plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
        "# Add a title\n",
        "plt.title('Vader - Needy Sentiment distribution')\n",
        "# Show the chart\n",
        "plt.show()\n",
        "\n",
        "print()\n",
        "\n",
        "# Flair - Needy Pie Chart Sentiment\n",
        "totalNeedy_Flair = len(positive_tweets_needy_flair) + len(negative_tweets_needy_flair)\n",
        "labels = ['Positive', 'Negative']\n",
        "sizes = [format(100 * len(positive_tweets_needy_flair) / totalNeedy_Flair), format(100 * len(negative_tweets_needy_flair) / totalNeedy_Flair)]\n",
        "colors = ['green', 'red']\n",
        "# Create a pie chart\n",
        "plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
        "# Add a title\n",
        "plt.title('Flair - Needy Sentiment distribution')\n",
        "# Show the chart\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2) Sentiment Distribution of Donors**"
      ],
      "metadata": {
        "id": "t2iulCZvPuYX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNF1H_qGyIkI"
      },
      "outputs": [],
      "source": [
        "# Text Blob - Donor Pie Chart Sentiment\n",
        "totalDonor_Textblob = len(positive_tweets_donor_textblob) + len(negative_tweets_donor_textblob)\n",
        "labels = ['Positive', 'Negative']\n",
        "sizes = [format(100 * len(positive_tweets_donor_textblob) / totalDonor_Textblob), format(100 * len(negative_tweets_donor_textblob) / totalDonor_Textblob)]\n",
        "colors = ['green', 'red']\n",
        "# Create a pie chart\n",
        "plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
        "# Add a title\n",
        "plt.title('TextBlob - Donor Sentiment distribution')\n",
        "# Show the chart\n",
        "plt.show()\n",
        "\n",
        "print()\n",
        "\n",
        "# Vader - Donor Pie Chart Sentiment\n",
        "totalDonor_Vader = len(positive_tweets_donor_vader) + len(negative_tweets_donor_vader)\n",
        "labels = ['Positive', 'Negative']\n",
        "sizes = [format(100 * len(positive_tweets_donor_vader) / totalDonor_Vader), format(100 * len(negative_tweets_donor_vader) / totalDonor_Vader)]\n",
        "colors = ['green', 'red']\n",
        "# Create a pie chart\n",
        "plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
        "# Add a title\n",
        "plt.title('Vader - Donor Sentiment distribution')\n",
        "# Show the chart\n",
        "plt.show()\n",
        "\n",
        "print()\n",
        "\n",
        "# Flair - Donor Pie Chart Sentiment\n",
        "totalDonor_Flair = len(positive_tweets_donor_flair) + len(negative_tweets_donor_flair)\n",
        "labels = ['Positive', 'Negative']\n",
        "sizes = [format(100 * len(positive_tweets_donor_flair) / totalDonor_Flair), format(100 * len(negative_tweets_donor_flair) / totalDonor_Flair)]\n",
        "colors = ['green', 'red']\n",
        "# Create a pie chart\n",
        "plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
        "# Add a title\n",
        "plt.title('Flair - Donor Sentiment distribution')\n",
        "# Show the chart\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**c) Choropleth Map**"
      ],
      "metadata": {
        "id": "D91YArR-Qn6n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1) Choropleth Map of Needy in USA**"
      ],
      "metadata": {
        "id": "YRfh6CxuQGbn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uyDp8C7QrcX1"
      },
      "outputs": [],
      "source": [
        "# List of all US States\n",
        "states = ['Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California', 'Colorado', 'Connecticut', 'Delaware', 'Florida', 'Georgia', 'Hawaii', 'Idaho', 'Illinois', 'Indiana', 'Iowa', 'Kansas', 'Kentucky', 'Louisiana', 'Maine', 'Maryland', 'Massachusetts', 'Michigan', 'Minnesota', 'Mississippi', 'Missouri', 'Montana', 'Nebraska', 'Nevada', 'New Hampshire', 'New Jersey', 'New Mexico', 'New York', 'North Carolina', 'North Dakota', 'Ohio', 'Oklahoma', 'Oregon', 'Pennsylvania', 'Rhode Island', 'South Carolina', 'South Dakota', 'Tennessee', 'Texas', 'Utah', 'Vermont', 'Virginia', 'Washington', 'West Virginia', 'Wisconsin', 'Wyoming']\n",
        "\n",
        "# List of all US state codes\n",
        "statesShortForm = ['AL', 'AK', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DE', 'FL', 'GA', 'HI', 'ID', 'IL', 'IN', 'IA', 'KS', 'KY', 'LA', 'ME', 'MD', 'MA', 'MI', 'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH', 'NJ', 'NM', 'NY', 'NC', 'ND', 'OH', 'OK', 'OR', 'PA', 'RI', 'SC', 'SD', 'TN', 'TX', 'UT', 'VT', 'VA', 'WA', 'WV', 'WI', 'WY']\n",
        "\n",
        "# For Needy - Flair\n",
        "# Create a defaultdict to store the count of needy tweets for each state\n",
        "stateNeedyCount = defaultdict(int)\n",
        "\n",
        "# Iterate over each needy tweet in positive_tweets_needy_vader\n",
        "for tweetId, tweet in positive_tweets_needy_vader.items():\n",
        "\n",
        "  # Get the location information from the tweet's data in needyTweets dictionary\n",
        "  loc = needyTweets[tweetId]['location']\n",
        "\n",
        "  # Check if the location string contains the name or state code for any US state\n",
        "  for i in range(50):\n",
        "    if states[i] in loc or statesShortForm[i] in loc:\n",
        "      # If the tweet belongs to a US state, increment the count for that state\n",
        "      stateNeedyCount[statesShortForm[i]] += 1\n",
        "      break\n",
        "\n",
        "# Create two empty lists to store the state codes and their respective counts\n",
        "states, countsvalue = [], []\n",
        "\n",
        "# Iterate over the stateNeedyCount defaultdict and store the keys (state codes) and values (count of needy tweets) in the above lists\n",
        "for key, value in stateNeedyCount.items():\n",
        "  states.append(key)\n",
        "  countsvalue.append(value)\n",
        "\n",
        "# Define the choropleth plot's data and layout parameters\n",
        "data = dict(type = 'choropleth',\n",
        "                locations = states,\n",
        "                locationmode = 'USA-states',\n",
        "                z = countsvalue,\n",
        "                colorscale = 'Greens',\n",
        "                colorbar = {'title' : 'colorbar'})\n",
        "layout = dict(title = 'Needy in USA',\n",
        "                geo = dict(scope='usa' ,\n",
        "                showlakes = True,\n",
        "                lakecolor = 'rgb(0,191,255)'))\n",
        "\n",
        "# Create a new figure object using the plotly.graph_objs module\n",
        "x = pg.Figure(data = [data], layout = layout)\n",
        "\n",
        "# Use the plotly.offline.iplot() method to plot the choropleth figure\n",
        "po.iplot(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2) Choropleth Map of Donors in USA**"
      ],
      "metadata": {
        "id": "4GCsv1MsQLmz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9y-7CjuX_BkM"
      },
      "outputs": [],
      "source": [
        "# List of all US States\n",
        "states = ['Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California', 'Colorado', 'Connecticut', 'Delaware', 'Florida', 'Georgia', 'Hawaii', 'Idaho', 'Illinois', 'Indiana', 'Iowa', 'Kansas', 'Kentucky', 'Louisiana', 'Maine', 'Maryland', 'Massachusetts', 'Michigan', 'Minnesota', 'Mississippi', 'Missouri', 'Montana', 'Nebraska', 'Nevada', 'New Hampshire', 'New Jersey', 'New Mexico', 'New York', 'North Carolina', 'North Dakota', 'Ohio', 'Oklahoma', 'Oregon', 'Pennsylvania', 'Rhode Island', 'South Carolina', 'South Dakota', 'Tennessee', 'Texas', 'Utah', 'Vermont', 'Virginia', 'Washington', 'West Virginia', 'Wisconsin', 'Wyoming']\n",
        "\n",
        "# List of all US state codes\n",
        "statesShortForm = ['AL', 'AK', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DE', 'FL', 'GA', 'HI', 'ID', 'IL', 'IN', 'IA', 'KS', 'KY', 'LA', 'ME', 'MD', 'MA', 'MI', 'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH', 'NJ', 'NM', 'NY', 'NC', 'ND', 'OH', 'OK', 'OR', 'PA', 'RI', 'SC', 'SD', 'TN', 'TX', 'UT', 'VT', 'VA', 'WA', 'WV', 'WI', 'WY']\n",
        "\n",
        "# For Donor - Flair\n",
        "stateDonorCount = defaultdict(int)\n",
        "\n",
        "# Iterate over each donor tweet in positive_tweets_donor_vader\n",
        "for tweetId, tweet in positive_tweets_donor_vader.items():\n",
        "\n",
        "  # Get the location information from the tweet's data in needyTweets dictionary\n",
        "  loc = donorTweets[tweetId]['location']\n",
        "\n",
        "  # Check if the location string contains the name or state code for any US state\n",
        "  for i in range(50):\n",
        "    if states[i] in loc or statesShortForm[i] in loc:\n",
        "      # If the tweet belongs to a US state, increment the count for that state\n",
        "      stateDonorCount[statesShortForm[i]] += 1\n",
        "      break\n",
        "\n",
        "# Create two empty lists to store the state codes and their respective counts\n",
        "states, countsvalue = [], []\n",
        "\n",
        "# Iterate over the stateDonorCount defaultdict and store the keys (state codes) and values (count of donor tweets) in the above lists\n",
        "for key, value in stateDonorCount.items():\n",
        "  states.append(key)\n",
        "  countsvalue.append(value)\n",
        "\n",
        "# Define the choropleth plot's data and layout parameters\n",
        "data = dict(type = 'choropleth',\n",
        "                locations = states,\n",
        "                locationmode = 'USA-states',\n",
        "                z = countsvalue,\n",
        "                colorscale = 'Greens',\n",
        "                colorbar = {'title' : 'colorbar'})\n",
        "layout = dict(title = 'Volunteers in USA',\n",
        "                geo = dict(scope='usa' ,\n",
        "                showlakes = True,\n",
        "                lakecolor = 'rgb(0,191,255)'))\n",
        "\n",
        "# Create a new figure object using the plotly.graph_objs module\n",
        "x = pg.Figure(data = [data], layout = layout)\n",
        "\n",
        "# Use the plotly.offline.iplot() method to plot the choropleth figure\n",
        "po.iplot(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_PHD4nXDB5G"
      },
      "source": [
        "# **Results**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1) Needy**"
      ],
      "metadata": {
        "id": "mDGBce_yQTnJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5SETGpBrA1I8"
      },
      "outputs": [],
      "source": [
        "# Create an empty list to store information about needy tweets.\n",
        "needyTweetList = []\n",
        "\n",
        "# Iterate over each needy tweet\n",
        "for tweetId, tweet in positive_tweets_needy_flair.items():\n",
        "  # Extract the full text of the tweet using its tweet ID from the needyTweets dictionary and append it as a key-value pair to the needyTweetList list.\n",
        "  needyTweetList.append({\n",
        "      'Needy Tweets': needyTweets[tweetId]['full_text']\n",
        "  })\n",
        "\n",
        "# Create a pandas DataFrame from the extracted information\n",
        "df = pd.DataFrame(needyTweetList)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df.to_csv(\"Needy Tweet Info.csv\", index=False)\n",
        "\n",
        "# Download the CSV file from the colab notebook.\n",
        "files.download('Needy Tweet Info.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2) Donors**"
      ],
      "metadata": {
        "id": "_Z7IS-iyQVvB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MyIxiUJMCAEs"
      },
      "outputs": [],
      "source": [
        "# Create an empty list to store information about donor tweets.\n",
        "donarTweetList = []\n",
        "\n",
        "# Iterate over each donor tweet\n",
        "for tweetId, tweet in positive_tweets_donor_flair.items():\n",
        "  # Extract the full text of the tweet using its tweet ID from the donorTweets dictionary and append it as a key-value pair to the donarTweetList list.\n",
        "  donarTweetList.append({\n",
        "      'Donor Tweets': donorTweets[tweetId]['full_text']\n",
        "  })\n",
        "\n",
        "# Create a pandas DataFrame from the extracted information\n",
        "df = pd.DataFrame(donarTweetList)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df.to_csv(\"Volunteer Tweet Info.csv\", index=False)\n",
        "\n",
        "# Download the CSV file from the colab notebook.\n",
        "files.download('Volunteer Tweet Info.csv')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}